{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (64, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=64x64 at 0x1F7057D1670>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "# Display image from path\n",
    "def display_image(image_path):\n",
    "    from PIL import Image\n",
    "    return Image.open(image_path)\n",
    "\n",
    "# Read an image to array\n",
    "def get_image_arr(image_pth):\n",
    "    return cv2.imread(image_pth)\n",
    "    \n",
    "# Transform an image to fit the inference model\n",
    "def image_transform(image, transform):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return transform(image=image)['image']\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(64, 64),\n",
    "    A.Normalize(0.449, 0.226)\n",
    "])\n",
    "\n",
    "# Define label map {id -> name}\n",
    "label2id = {'non-smile':0, 'smile':1}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "labels = list(label2id.keys())\n",
    "\n",
    "image_path = './static/image/1.jpg'\n",
    "image = get_image_arr(image_path)\n",
    "image = image_transform(image, transform)\n",
    "print(\"Image shape\", image.shape)\n",
    "\n",
    "display_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get device and convert image array to tensor\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "image = torch.tensor(image).unsqueeze(0).unsqueeze(0).to(device)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5601, -0.5327]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the inference model\n",
    "from inference.model import *\n",
    "inference_model = torch.load('static/saved_models/vgg_inference.pt')\n",
    "inference_model.eval()\n",
    "inference_model.to(device)\n",
    "\n",
    "# Warm up\n",
    "inference_model(torch.randn(1,1,64,64).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'non-smile': 0.5134626030921936, 'smile': 0.4885599613189697}\n"
     ]
    }
   ],
   "source": [
    "# Get outputs and normalize\n",
    "probs = torch.sigmoid(inference_model(image))[0].tolist()\n",
    "probs_map = {k:v for k, v in zip(labels, probs)}\n",
    "print(probs_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "511e198c7044f4fa1c9fb6786af37bc0803f028a2364c2b1dc987f96133f7904"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
